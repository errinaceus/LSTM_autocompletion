{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bdd9759",
   "metadata": {},
   "source": [
    "## Сбор и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f623cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/k305-1/LEGEND_960_2/python/ac/venv2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import pipeline\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('src'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bb1383",
   "metadata": {},
   "source": [
    "### очистка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fec571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1277164, val: 159646, test: 159646\n"
     ]
    }
   ],
   "source": [
    "import preprocess\n",
    "prepare_dataset()\n",
    "split_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ad9859",
   "metadata": {},
   "source": [
    "### токенизация, разбиение на тренировачный, валидационный и тестовый датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1080ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "from tqdm import tqdm\n",
    "from next_token_dataset import *\n",
    "\n",
    "# Загружаем BERT токенизатор\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "train_texts = pd.read_csv(\"data/train.csv\")\n",
    "train_texts=train_texts['text'].tolist() \n",
    "val_texts = pd.read_csv(\"data/val.csv\")\n",
    "val_texts = val_texts['text'].tolist() \n",
    "# тренировочный и валидационный датасеты\n",
    "train_dataset = NextTokenDataset(train_texts, tokenizer, max_length=20)\n",
    "val_dataset = NextTokenDataset(val_texts, tokenizer, max_length=20)\n",
    "\n",
    "# даталоадеры\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=lambda batch: collate_fn_pad_sequence(batch, tokenizer))\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, collate_fn=lambda batch: collate_fn_pad_sequence(batch, tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee61a254",
   "metadata": {},
   "source": [
    "## объявление модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffe02d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm_train import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99db7f4",
   "metadata": {},
   "source": [
    "Объявлена модель на основе архитектуры LSTM со следующими параметрами:\n",
    "embed_dim=128, hidden_dim=256, num_layers=2, dropout=0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45371a04",
   "metadata": {},
   "source": [
    "## обучение LSTM модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fe5ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# объявляем модель\n",
    "model = LSTMTextGenerator(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=128,\n",
    "    hidden_dim=256,  \n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "# Обучение\n",
    "train_losses, val_losses = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    epochs=10,\n",
    "    lr=0.001,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "# Сохранение\n",
    "save_checkpoint(\n",
    "    model=model,\n",
    "    optimizer=optim.Adam(model.parameters(), lr=0.001),\n",
    "    epoch=10,\n",
    "    train_loss=train_losses[-1],\n",
    "    val_loss=val_losses[-1],\n",
    "    filename='lstm_text_generator_final.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b9e943",
   "metadata": {},
   "source": [
    "Модель обучена в течение 10 эпох. Финальные метрики обучения:  Train Loss: 5.3009, Val Loss: 5.1672"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c334b45",
   "metadata": {},
   "source": [
    "## запуск LSTM-модели на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ffc823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "модель загружена lstm_text_generator_final.pt\n",
      "Epoch: 10\n",
      "Train Loss: 5.2689\n",
      "Val Loss: 5.1347\n",
      "\n",
      "1. Автодополнение...\n",
      "Evaluating model on test dataset...\n",
      "Total batches: 63638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   2%|▏         | 1049/63638 [00:03<03:53, 268.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 batches, current avg ROUGE-1: 0.2206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   3%|▎         | 2035/63638 [00:07<03:48, 269.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2000 batches, current avg ROUGE-1: 0.2224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   5%|▍         | 3053/63638 [00:11<03:48, 265.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3000 batches, current avg ROUGE-1: 0.2210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   6%|▋         | 4045/63638 [00:15<03:41, 269.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4000 batches, current avg ROUGE-1: 0.2219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   8%|▊         | 5042/63638 [00:18<03:38, 268.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5000 batches, current avg ROUGE-1: 0.2217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   9%|▉         | 6039/63638 [00:22<03:36, 265.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6000 batches, current avg ROUGE-1: 0.2210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  11%|█         | 7027/63638 [00:26<03:32, 266.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  13%|█▎        | 8028/63638 [00:29<03:25, 271.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8000 batches, current avg ROUGE-1: 0.2206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  14%|█▍        | 9043/63638 [00:33<03:20, 272.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 9000 batches, current avg ROUGE-1: 0.2207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  16%|█▌        | 10052/63638 [00:37<03:17, 270.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 batches, current avg ROUGE-1: 0.2204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  17%|█▋        | 11037/63638 [00:40<03:16, 267.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 11000 batches, current avg ROUGE-1: 0.2205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  19%|█▉        | 12045/63638 [00:44<03:08, 273.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 12000 batches, current avg ROUGE-1: 0.2201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  20%|██        | 13032/63638 [00:48<03:05, 273.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 13000 batches, current avg ROUGE-1: 0.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  22%|██▏       | 14046/63638 [00:51<03:02, 271.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 14000 batches, current avg ROUGE-1: 0.2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  24%|██▎       | 15031/63638 [00:55<02:59, 271.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 15000 batches, current avg ROUGE-1: 0.2199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  25%|██▌       | 16040/63638 [00:59<02:56, 269.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 16000 batches, current avg ROUGE-1: 0.2199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  27%|██▋       | 17052/63638 [01:02<02:51, 270.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 17000 batches, current avg ROUGE-1: 0.2199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  28%|██▊       | 18038/63638 [01:06<02:48, 270.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 18000 batches, current avg ROUGE-1: 0.2203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  30%|██▉       | 19051/63638 [01:09<02:45, 269.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 19000 batches, current avg ROUGE-1: 0.2203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  31%|███▏      | 20033/63638 [01:13<02:44, 265.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20000 batches, current avg ROUGE-1: 0.2203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  33%|███▎      | 21043/63638 [01:17<02:43, 261.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 21000 batches, current avg ROUGE-1: 0.2204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  35%|███▍      | 22045/63638 [01:21<02:37, 263.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 22000 batches, current avg ROUGE-1: 0.2207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  36%|███▌      | 23042/63638 [01:24<02:36, 259.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 23000 batches, current avg ROUGE-1: 0.2206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  38%|███▊      | 24040/63638 [01:28<02:32, 259.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 24000 batches, current avg ROUGE-1: 0.2206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  39%|███▉      | 25053/63638 [01:32<02:29, 258.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 25000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  41%|████      | 26041/63638 [01:36<02:24, 260.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 26000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  42%|████▏     | 27038/63638 [01:39<02:20, 260.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 27000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  44%|████▍     | 28035/63638 [01:43<02:22, 249.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 28000 batches, current avg ROUGE-1: 0.2209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  46%|████▌     | 29034/63638 [01:47<02:19, 248.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 29000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  47%|████▋     | 30036/63638 [01:51<02:08, 261.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  49%|████▉     | 31036/63638 [01:54<02:05, 259.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 31000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  50%|█████     | 32033/63638 [01:58<02:00, 261.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 32000 batches, current avg ROUGE-1: 0.2209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  52%|█████▏    | 33034/63638 [02:02<01:56, 261.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 33000 batches, current avg ROUGE-1: 0.2209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  53%|█████▎    | 34039/63638 [02:05<01:53, 261.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 34000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  55%|█████▌    | 35043/63638 [02:09<01:50, 259.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 35000 batches, current avg ROUGE-1: 0.2207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  57%|█████▋    | 36044/63638 [02:13<01:45, 261.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 36000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  58%|█████▊    | 37048/63638 [02:17<01:43, 258.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 37000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  60%|█████▉    | 38041/63638 [02:20<01:39, 257.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 38000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  61%|██████▏   | 39034/63638 [02:24<01:35, 256.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 39000 batches, current avg ROUGE-1: 0.2209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  63%|██████▎   | 40052/63638 [02:28<01:30, 259.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40000 batches, current avg ROUGE-1: 0.2207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  64%|██████▍   | 41029/63638 [02:31<01:28, 254.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 41000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  66%|██████▌   | 42030/63638 [02:35<01:23, 257.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 42000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  68%|██████▊   | 43030/63638 [02:39<01:20, 256.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 43000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  69%|██████▉   | 44047/63638 [02:43<01:16, 257.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 44000 batches, current avg ROUGE-1: 0.2207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  71%|███████   | 45047/63638 [02:46<01:12, 256.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 45000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  72%|███████▏  | 46048/63638 [02:50<01:08, 257.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 46000 batches, current avg ROUGE-1: 0.2207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  74%|███████▍  | 47044/63638 [02:54<01:04, 257.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 47000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  75%|███████▌  | 48039/63638 [02:58<01:00, 256.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 48000 batches, current avg ROUGE-1: 0.2209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  77%|███████▋  | 49043/63638 [03:01<00:56, 258.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 49000 batches, current avg ROUGE-1: 0.2210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  79%|███████▊  | 50046/63638 [03:05<00:52, 258.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50000 batches, current avg ROUGE-1: 0.2209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  80%|████████  | 51052/63638 [03:09<00:49, 256.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 51000 batches, current avg ROUGE-1: 0.2209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  82%|████████▏ | 52047/63638 [03:13<00:45, 253.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 52000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  83%|████████▎ | 53041/63638 [03:16<00:41, 254.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 53000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  85%|████████▍ | 54044/63638 [03:20<00:37, 254.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 54000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  86%|████████▋ | 55034/63638 [03:24<00:34, 245.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 55000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  88%|████████▊ | 56030/63638 [03:28<00:30, 251.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 56000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  90%|████████▉ | 57049/63638 [03:31<00:26, 253.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 57000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  91%|█████████ | 58034/63638 [03:35<00:23, 243.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 58000 batches, current avg ROUGE-1: 0.2209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  93%|█████████▎| 59032/63638 [03:39<00:18, 252.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 59000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  94%|█████████▍| 60031/63638 [03:43<00:14, 248.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 60000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  96%|█████████▌| 61043/63638 [03:46<00:10, 250.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 61000 batches, current avg ROUGE-1: 0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  97%|█████████▋| 62034/63638 [03:50<00:06, 251.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 62000 batches, current avg ROUGE-1: 0.2209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  99%|█████████▉| 63052/63638 [03:54<00:02, 249.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 63000 batches, current avg ROUGE-1: 0.2209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 63638/63638 [03:56<00:00, 268.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Результаты предсказания текста          \n",
      "ROUGE-1: 0.2209 (±0.4148)\n",
      "ROUGE-2: 0.0000 (±0.0000)\n",
      "Number of samples: 2036384\n",
      "\n",
      "Примеры:\n",
      "Reference: 'i'\n",
      "Generated: 'i'\n",
      "\n",
      "Reference: 'm'\n",
      "Generated: 'm'\n",
      "\n",
      "Reference: 'hell'\n",
      "Generated: 'not'\n",
      "\n",
      "Reference: '##a'\n",
      "Generated: '##a'\n",
      "\n",
      "Reference: 'bored'\n",
      "Generated: 'tired'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from eval_lstm import *\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  \n",
    "\n",
    "\n",
    "# загрузка модели\n",
    "\n",
    "model, checkpoint = load_model(\n",
    "    checkpoint_path='lstm_text_generator_final.pt',\n",
    "    vocab_size=30522,  # BERT base uncased vocabulary size\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "# создаем test dataset\n",
    "test_texts = pd.read_csv(\"data/test.csv\")['text'].tolist()\n",
    "\n",
    "\n",
    "test_dataset = NextTokenDataset(test_texts, tokenizer, max_length=20)\n",
    "\n",
    "print(\"\\n1. Автодополнение...\")\n",
    "token_results, gen_texts, ref_texts = evaluate_model(\n",
    "    model=model,\n",
    "    test_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print_evaluation_results(token_results, \"Результаты предсказания текста\")\n",
    "\n",
    "print(\"\\nПримеры:\")\n",
    "for i in range(min(5, len(gen_texts))):\n",
    "    print(f\"Reference: '{ref_texts[i]}'\")\n",
    "    print(f\"Generated: '{gen_texts[i]}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de0b55",
   "metadata": {},
   "source": [
    "### примеры предсказаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b3ff7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Примеры:\n",
      "Prompt: 'well i m hella' -> 'well i m hella i m so sad i can t wait to see him and i'\n",
      "Prompt: 'i am so excited' -> 'i am so excited i m so sad i can t wait to see you and i m'\n",
      "Prompt: 'once upon' -> 'once upon the one i can t do it i m so sad to be the best of'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_prompts = [\"well i m hella\", \"i am so excited\", \"once upon\"]\n",
    "print(\"\\nПримеры:\")\n",
    "for prompt in sample_prompts:\n",
    "    generated = model.generate(tokenizer, prompt, max_length=20, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Prompt: '{prompt}' -> '{generated}'\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d024c448",
   "metadata": {},
   "source": [
    "## Запуск предобученного трансформера на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82c71d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 159646 test texts\n",
      "Loading distilgpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   0%|          | 0/159646 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Generating:   6%|▋         | 10004/159646 [07:44<1:55:09, 21.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 10000 samples, Avg ROUGE-1: 0.0655, Avg ROUGE-2: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  13%|█▎        | 20004/159646 [15:24<1:41:24, 22.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20000 samples, Avg ROUGE-1: 0.0656, Avg ROUGE-2: 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  19%|█▉        | 30004/159646 [23:05<1:41:49, 21.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 30000 samples, Avg ROUGE-1: 0.0652, Avg ROUGE-2: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  25%|██▌       | 40005/159646 [30:44<1:30:24, 22.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 40000 samples, Avg ROUGE-1: 0.0653, Avg ROUGE-2: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  31%|███▏      | 50004/159646 [38:26<1:10:43, 25.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 50000 samples, Avg ROUGE-1: 0.0654, Avg ROUGE-2: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  38%|███▊      | 60003/159646 [46:15<1:08:43, 24.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 60000 samples, Avg ROUGE-1: 0.0653, Avg ROUGE-2: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  44%|████▍     | 70002/159646 [54:02<1:09:17, 21.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 70000 samples, Avg ROUGE-1: 0.0652, Avg ROUGE-2: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  50%|█████     | 80003/159646 [1:01:41<1:10:51, 18.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 80000 samples, Avg ROUGE-1: 0.0652, Avg ROUGE-2: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  56%|█████▋    | 90002/159646 [1:09:21<1:02:48, 18.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 90000 samples, Avg ROUGE-1: 0.0652, Avg ROUGE-2: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  63%|██████▎   | 100006/159646 [1:17:06<35:49, 27.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 100000 samples, Avg ROUGE-1: 0.0651, Avg ROUGE-2: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  69%|██████▉   | 110004/159646 [1:24:46<38:48, 21.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 110000 samples, Avg ROUGE-1: 0.0652, Avg ROUGE-2: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  75%|███████▌  | 120001/159646 [1:32:26<27:23, 24.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 120000 samples, Avg ROUGE-1: 0.0652, Avg ROUGE-2: 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  81%|████████▏ | 130004/159646 [1:40:06<20:22, 24.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 130000 samples, Avg ROUGE-1: 0.0652, Avg ROUGE-2: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  88%|████████▊ | 140004/159646 [1:47:45<15:46, 20.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 140000 samples, Avg ROUGE-1: 0.0654, Avg ROUGE-2: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 159646/159646 [2:02:49<00:00, 21.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики DistilGPT2\n",
      "ROUGE-1: 0.0652 (±0.0773)\n",
      "ROUGE-2: 0.0039 (±0.0209)\n"
     ]
    }
   ],
   "source": [
    "from eval_transformer_pipeline import *\n",
    "#  test датасет\n",
    "test_texts = pd.read_csv(\"data/test.csv\")['text'].tolist()\n",
    "print(f\"Loaded {len(test_texts)} test texts\")\n",
    "\n",
    "\n",
    "\n",
    "gpt_results, gpt_examples = evaluate_distilgpt(\n",
    "        model_name=\"distilgpt2\",\n",
    "        test_texts=test_texts,\n",
    "        prompt_length=5,\n",
    "        max_new_tokens=30,\n",
    "        batch_size=8,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Метрики DistilGPT2\")\n",
    "print(f\"ROUGE-1: {gpt_results['rouge1']:.4f} (±{gpt_results['rouge1_std']:.4f})\")\n",
    "print(f\"ROUGE-2: {gpt_results['rouge2']:.4f} (±{gpt_results['rouge2_std']:.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1480df",
   "metadata": {},
   "source": [
    "### примеры предсказаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45dbe57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Примеры\n",
      "\n",
      "Example 1:\n",
      "Prompt: 'well i m hella'\n",
      "Reference: ' bored here and i hate that school is just a week away'\n",
      "Generated: '.\n",
      "I have to admit I am quite a bit of a fan of the game but the overall experience was fantastic. The game is as good as'\n",
      "ROUGE-1: 0.1538\n",
      "\n",
      "Example 2:\n",
      "Prompt: 'is so excited as c'\n",
      "Reference: 'g has the package now i have to wait until she comes on msn'\n",
      "Generated: 'uz you've got to make the most of it.\n",
      "\n",
      "\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-'\n",
      "ROUGE-1: 0.1667\n",
      "\n",
      "Example 3:\n",
      "Prompt: 'just went shopping but i'\n",
      "Reference: ' m still bummed about no wrestling for me no triple h'\n",
      "Generated: ''m not sure how to keep my money on the exchanges for the entire month! Thanks a lot for your support!'\n",
      "ROUGE-1: 0.1250\n",
      "\n",
      "Example 4:\n",
      "Prompt: 'wish i could by'\n",
      "Reference: ' music from itunes in brazil we re not allowed here you know'\n",
      "Generated: 'the way i am now in the UK.\n",
      "\n",
      "\n",
      "It was pretty much the only thing I can remember about the phone's firmware. I remember'\n",
      "ROUGE-1: 0.0541\n",
      "\n",
      "Example 5:\n",
      "Prompt: 'listening to geh'\n",
      "Reference: ' tokio hotel love that music lt 3'\n",
      "Generated: 'oc'\n",
      "ROUGE-1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Примеры\")\n",
    "for i, ex in enumerate(gpt_examples[:5]):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Prompt: '{ex['prompt']}'\")\n",
    "    print(f\"Reference: '{ex['reference']}'\")\n",
    "    print(f\"Generated: '{ex['generated']}'\")\n",
    "    print(f\"ROUGE-1: {ex['rouge1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c1bcba",
   "metadata": {},
   "source": [
    "## Сравнение моделей\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aab6d99",
   "metadata": {},
   "source": [
    "Метрики LSTM\n",
    "ROUGE-1: 0.2209 (±0.4148)\n",
    "ROUGE-2: 0.0000 (±0.0000)\n",
    "\n",
    "Примеры предсказаний:\n",
    "Prompt1: 'well i m hella' \n",
    "Generated: 'well i m hella i m so sad i can t wait to see him and i'\n",
    "\n",
    "Prompt2: 'i am so excited'\n",
    "Generated: 'i am so excited i m so sad i can t wait to see you and i m'\n",
    "\n",
    "Prompt3: 'once upon' \n",
    "Generated: 'once upon the one i can t do it i m so sad to be the best of'\n",
    "\n",
    "\n",
    "Метрики DistilGPT2\n",
    "ROUGE-1: 0.0655 (±0.0776)\n",
    "ROUGE-2: 0.0039 (±0.0210)\n",
    "\n",
    "\n",
    "Prompt1: 'well i m hella'\n",
    "Generated: 'I have to admit I am quite a bit of a fan of the game but the overall experience was fantastic. The game is as good as'\n",
    "\n",
    "Prompt2: 'is so excited as c'\n",
    "Generated: 'uz you've got to make the most of it.'\n",
    "\n",
    "Prompt3: 'just went shopping but i'\n",
    "Generated: ''m not sure how to keep my money on the exchanges for the entire month! Thanks a lot for your support!'\n",
    "\n",
    "Prompt4: 'wish i could by'\n",
    "Generated: 'the way i am now in the UK. It was pretty much the only thing I can remember about the phone's firmware. I remember'\n",
    "\n",
    "Вывод: обученная нами RNN-модель на основе LSTM генерирует связные, но бессмыссленные фрагменты текста. Совпадение биграмм (ROUGE-2) 0. По всей видимости, требуется дальнейшая оптимизация модели и тренировочных датасетов.  Модель DistilGPT2 в некоторых случаях тоже генерирует бессмысленные повторяющиеся слова, однако на большинство запросов генерирует логичные фрагменты текста, вполне осмысленные в контексте промта. При этом метрики ROUGE имеют очень малые значения, однако по всей видимости это отражает то, что модель была обучена на других данных и способна генерировать принципиально другой ответ, отличный от имеющегося в тестовом датасете, что и отражается на значениях метрик.\n",
    "\n",
    "Время инференса DistilGPT2 на тестовом датасете 2:02:49\n",
    "Время инференса LSTM на тестовом датасете 03:56\n",
    "Таким образом, LSTM более чем 30 раз быстрее DistilGPT2, что также важно при запуске в условиях ограниченных ресурсов/времени.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
